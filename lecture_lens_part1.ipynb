{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":11509292,"datasetId":7216665}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:44:50.962507Z","iopub.execute_input":"2025-04-22T05:44:50.963240Z","iopub.status.idle":"2025-04-22T05:44:50.996347Z","shell.execute_reply.started":"2025-04-22T05:44:50.963212Z","shell.execute_reply":"2025-04-22T05:44:50.995591Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llm-data/LLM_DATASET/stable_diffusion/19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).mp4\n/kaggle/input/llm-data/LLM_DATASET/stable_diffusion/Stable Diffusion.pdf\n/kaggle/input/llm-data/LLM_DATASET/stable_diffusion/19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4\n/kaggle/input/llm-data/LLM_DATASET/Lora&Qlora/19853_shylaja.sharath_31_20250318125700082_Video_ENC.mp4\n/kaggle/input/llm-data/LLM_DATASET/Lora&Qlora/Finetuning.pdf\n/kaggle/input/llm-data/LLM_DATASET/Lora&Qlora/19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).mp4\n/kaggle/input/llm-data/LLM_DATASET/Lora&Qlora/19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).mp4\n/kaggle/input/llm-data/LLM_DATASET/expn_tree/7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.mkv\n/kaggle/input/llm-data/LLM_DATASET/expn_tree/Class7_Unit3_Trees_ExprTree.pptx\n/kaggle/input/llm-data/LLM_DATASET/expn_tree/7a_2020-09-24 09-28-52_ExprTreeCon.mkv\n/kaggle/input/llm-data/LLM_DATASET/agentic/AutoGen CrewAI.pdf\n/kaggle/input/llm-data/LLM_DATASET/agentic/19853_shylaja.sharath_31_20250401121200417_Video_ENC.mp4\n/kaggle/input/llm-data/LLM_DATASET/agentic/Agentic Workflow.pdf\n/kaggle/input/llm-data/LLM_DATASET/Heap/8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_121.mkv\n/kaggle/input/llm-data/LLM_DATASET/Heap/Class8_Unit3_Trees_Heap.pptx\n/kaggle/input/llm-data/LLM_DATASET/Heap/8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.mkv\n/kaggle/input/llm-data/LLM_DATASET/BST/Class2_Unit3_Tree_BST_DynamicInsert.pptx\n/kaggle/input/llm-data/LLM_DATASET/BST/Class3_Unit3_Trees_BSTDeletion.pptx\n/kaggle/input/llm-data/LLM_DATASET/BST/Class4_Unit3_Trees_BST_ArrayInsert.pptx\n/kaggle/input/llm-data/LLM_DATASET/TBT/Class6_Unit3_Trees_ThreadBST.pptx\n/kaggle/input/llm-data/LLM_DATASET/TBT/6a_2020-09-22 09-49-32_TBTCon.mkv\n/kaggle/input/llm-data/LLM_DATASET/TBT/6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.mkv\n/kaggle/input/llm-data/LLM_DATASET/Multimodal/19853_shylaja.sharath_31_20250401112700078_Video_ENC.mp4\n/kaggle/input/llm-data/LLM_DATASET/Multimodal/MAMBA.pdf\n/kaggle/input/llm-data/LLM_DATASET/Multimodal/MultiModal LLMs.pdf\n/kaggle/input/llm-data/LLM_DATASET/binary_tree_traversal/5a_2020-09-15 09-04-51_BinTraversal.mkv\n/kaggle/input/llm-data/LLM_DATASET/binary_tree_traversal/Class5_Unit3_BST_Traversal.pptx\n/kaggle/input/llm-data/LLM_DATASET/Tree_traversal/class9_Unit3_Trees_naryTraversal.pptx\n/kaggle/input/llm-data/LLM_DATASET/Tree_traversal/9a_2020-09-16 09-46-13_TreeTravCon.mkv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Step 1: Setting Up the Kaggle Environment and Organizing Data","metadata":{}},{"cell_type":"code","source":"# Step 1: Environment Setup and Data Verification\n\nimport os\nimport subprocess\nimport torch\nfrom pathlib import Path\n\n# 1. Install necessary system packages (like ffmpeg for audio processing)\n# Using 'apt-get update' first to ensure package lists are fresh\nprint(\"Updating package lists...\")\nupdate_process = subprocess.run(['apt-get', 'update', '-qq'], capture_output=True, text=True)\nif update_process.returncode != 0:\n    print(\"Warning: apt-get update failed. Proceeding with install anyway.\")\n    # print(\"Update Error:\", update_process.stderr) # Uncomment for detailed error\n\nprint(\"Installing ffmpeg...\")\ninstall_process = subprocess.run(['apt-get', 'install', '-y', '-qq', 'ffmpeg'], capture_output=True, text=True)\nif install_process.returncode == 0:\n    print(\"ffmpeg installed successfully.\")\nelse:\n    print(\"Error installing ffmpeg:\")\n    print(install_process.stderr)\n    # Consider adding alternative installation or raising an error if ffmpeg is critical\n\n# 2. Check for GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(f\"GPU is available. Using device: {torch.cuda.get_device_name(0)}\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available. Using CPU. Note: Processing will be significantly slower.\")\n\n# 3. Define the path to your uploaded dataset\n# Adjust 'lecture-data' if you named your uploaded dataset differently\n# Adjust 'LLM_DATASET' if the root folder inside the zip had a different name\ndataset_base_path = Path(\"/kaggle/input/llm-data/LLM_DATASET\")\n\n# 4. Verify dataset structure\nif dataset_base_path.exists() and dataset_base_path.is_dir():\n    print(f\"\\nFound dataset directory: {dataset_base_path}\")\n    print(\"Contents (Subject Folders):\")\n    subject_folders = [f.name for f in dataset_base_path.iterdir() if f.is_dir()]\n    print(subject_folders)\n\n    # Optional: List contents of the first subject folder to see files\n    if subject_folders:\n        first_subject_path = dataset_base_path / subject_folders[0]\n        print(f\"\\nContents of '{subject_folders[0]}':\")\n        try:\n            for item in first_subject_path.iterdir():\n                print(f\"- {item.name} ({'Dir' if item.is_dir() else 'File'})\")\n        except PermissionError:\n            print(f\"Could not access contents of {first_subject_path} due to permissions.\")\n        except Exception as e:\n            print(f\"An error occurred while listing contents of {first_subject_path}: {e}\")\n    else:\n        print(\"No subject folders found in the dataset directory.\")\n\nelse:\n    print(f\"\\nError: Dataset directory not found at {dataset_base_path}\")\n    print(\"Please check:\")\n    print(\"1. If you uploaded the data correctly.\")\n    print(\"2. If the dataset name in Kaggle matches 'lecture-data'.\")\n    print(\"3. If the root folder inside your zip file was 'LLM_DATASET'.\")\n    # You might need to list /kaggle/input/ to see the actual structure:\n    # print(\"\\nAvailable contents in /kaggle/input/:\")\n    # for item in Path(\"/kaggle/input/\").iterdir():\n    #     print(f\"- {item.name}\")\n\n\n# 5. Create an output directory for results\noutput_base_path = Path(\"/kaggle/working/output\")\noutput_base_path.mkdir(parents=True, exist_ok=True)\nprint(f\"\\nCreated output directory: {output_base_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:44:52.905054Z","iopub.execute_input":"2025-04-22T05:44:52.905644Z","iopub.status.idle":"2025-04-22T05:44:57.538099Z","shell.execute_reply.started":"2025-04-22T05:44:52.905620Z","shell.execute_reply":"2025-04-22T05:44:57.537297Z"}},"outputs":[{"name":"stdout","text":"Updating package lists...\nInstalling ffmpeg...\nffmpeg installed successfully.\nGPU is available. Using device: Tesla T4\n\nFound dataset directory: /kaggle/input/llm-data/LLM_DATASET\nContents (Subject Folders):\n['stable_diffusion', 'Lora&Qlora', 'expn_tree', 'agentic', 'Heap', 'BST', 'TBT', 'Multimodal', 'binary_tree_traversal', 'Tree_traversal']\n\nContents of 'stable_diffusion':\n- 19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).mp4 (File)\n- Stable Diffusion.pdf (File)\n- 19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4 (File)\n\nCreated output directory: /kaggle/working/output\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Step 2: Audio Extraction from Videos","metadata":{}},{"cell_type":"code","source":"# Step 2: Audio Extraction\n\nimport os\nimport subprocess\nfrom pathlib import Path\nimport time\n\n# Base paths defined in Step 1\n# dataset_base_path = Path(\"/kaggle/input/llm-data/LLM_DATASET\")\n# output_base_path = Path(\"/kaggle/working/output\")\n\n# Supported video extensions\nvideo_extensions = ['.mp4', '.mkv']\n\ndef extract_audio(video_path, output_audio_path):\n    \"\"\"\n    Extracts audio from a video file using ffmpeg, saves as WAV.\n    Standardizes to 16kHz mono PCM audio.\n    \"\"\"\n    command = [\n        'ffmpeg',\n        '-i', str(video_path),        # Input file\n        '-vn',                        # Disable video recording\n        '-acodec', 'pcm_s16le',       # Audio codec: PCM signed 16-bit little-endian (standard WAV)\n        '-ar', '16000',               # Audio sample rate: 16kHz\n        '-ac', '1',                   # Audio channels: 1 (mono)\n        '-y',                         # Overwrite output file if it exists\n        '-hide_banner',               # Hide unnecessary console output\n        '-loglevel', 'error',         # Show only errors\n        str(output_audio_path)        # Output file\n    ]\n    try:\n        print(f\"  Extracting audio from: {video_path.name}\")\n        start_time = time.time()\n        result = subprocess.run(command, check=True, capture_output=True, text=True)\n        end_time = time.time()\n        print(f\"  Successfully extracted to: {output_audio_path.name} (took {end_time - start_time:.2f}s)\")\n        return True\n    except subprocess.CalledProcessError as e:\n        print(f\"  Error extracting audio from {video_path.name}:\")\n        print(f\"  Command: {' '.join(command)}\") # Show the command that failed\n        print(f\"  FFmpeg Error Output:\\n{e.stderr}\")\n        # Attempt to clean up potentially incomplete output file\n        if output_audio_path.exists():\n            try:\n                output_audio_path.unlink()\n                print(f\"  Cleaned up incomplete file: {output_audio_path.name}\")\n            except OSError as unlink_err:\n                print(f\"  Warning: Could not delete incomplete file {output_audio_path.name}: {unlink_err}\")\n        return False\n    except FileNotFoundError:\n        print(\" Error: ffmpeg command not found. Make sure ffmpeg is installed and in the system's PATH.\")\n        # This shouldn't happen if Step 1 was successful, but good to check.\n        return False\n    except Exception as e:\n        print(f\"  An unexpected error occurred during extraction from {video_path.name}: {e}\")\n        return False\n\n# --- Main Extraction Loop ---\ntotal_videos = 0\nsuccessful_extractions = 0\nfailed_extractions = 0\n\nprint(\"\\n--- Starting Audio Extraction ---\")\n\n# Iterate through subject folders in the dataset\nfor subject_dir in dataset_base_path.iterdir():\n    if subject_dir.is_dir():\n        print(f\"\\nProcessing subject: {subject_dir.name}\")\n\n        # Create corresponding output directory structure for audio\n        output_subject_audio_dir = output_base_path / subject_dir.name / \"audio\"\n        output_subject_audio_dir.mkdir(parents=True, exist_ok=True)\n\n        # Find video files in the current subject directory\n        video_files = [f for f in subject_dir.iterdir()\n                       if f.is_file() and f.suffix.lower() in video_extensions]\n\n        if not video_files:\n            print(f\"  No video files ({', '.join(video_extensions)}) found in {subject_dir.name}.\")\n            continue\n\n        for video_file in video_files:\n            total_videos += 1\n            # Define the output audio file path\n            output_audio_file = output_subject_audio_dir / f\"{video_file.stem}.wav\"\n\n            # Extract audio\n            if extract_audio(video_file, output_audio_file):\n                successful_extractions += 1\n            else:\n                failed_extractions += 1\n\nprint(\"\\n--- Audio Extraction Summary ---\")\nprint(f\"Total videos found: {total_videos}\")\nprint(f\"Successfully extracted audio: {successful_extractions}\")\nprint(f\"Failed extractions: {failed_extractions}\")\n\n# Optional: Verify by listing some output files\nif successful_extractions > 0:\n    print(\"\\nExample output audio files:\")\n    example_count = 0\n    for subject_dir in output_base_path.iterdir():\n        if subject_dir.is_dir() and (subject_dir / \"audio\").exists():\n             for audio_file in (subject_dir / \"audio\").iterdir():\n                 if audio_file.suffix == '.wav' and example_count < 5:\n                     print(f\"- {audio_file.relative_to(output_base_path)}\")\n                     example_count += 1\n             if example_count >= 5:\n                 break\n    if example_count == 0:\n        print(\"Could not find any example .wav files in the output directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:45:02.850389Z","iopub.execute_input":"2025-04-22T05:45:02.850885Z","iopub.status.idle":"2025-04-22T05:46:31.278731Z","shell.execute_reply.started":"2025-04-22T05:45:02.850861Z","shell.execute_reply":"2025-04-22T05:46:31.278095Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Audio Extraction ---\n\nProcessing subject: stable_diffusion\n  Extracting audio from: 19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).mp4\n  Successfully extracted to: 19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).wav (took 2.24s)\n  Extracting audio from: 19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4\n  Successfully extracted to: 19853_shylaja.sharath_31_20250327084200249_Video_ENC.wav (took 2.05s)\n\nProcessing subject: Lora&Qlora\n  Extracting audio from: 19853_shylaja.sharath_31_20250318125700082_Video_ENC.mp4\n  Successfully extracted to: 19853_shylaja.sharath_31_20250318125700082_Video_ENC.wav (took 3.01s)\n  Extracting audio from: 19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).mp4\n  Successfully extracted to: 19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).wav (took 2.82s)\n  Extracting audio from: 19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).mp4\n  Successfully extracted to: 19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).wav (took 2.77s)\n\nProcessing subject: expn_tree\n  Extracting audio from: 7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.mkv\n  Successfully extracted to: 7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.wav (took 7.00s)\n  Extracting audio from: 7a_2020-09-24 09-28-52_ExprTreeCon.mkv\n  Successfully extracted to: 7a_2020-09-24 09-28-52_ExprTreeCon.wav (took 10.89s)\n\nProcessing subject: agentic\n  Extracting audio from: 19853_shylaja.sharath_31_20250401121200417_Video_ENC.mp4\n  Successfully extracted to: 19853_shylaja.sharath_31_20250401121200417_Video_ENC.wav (took 2.36s)\n\nProcessing subject: Heap\n  Extracting audio from: 8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_121.mkv\n  Successfully extracted to: 8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_121.wav (took 11.09s)\n  Extracting audio from: 8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.mkv\n  Successfully extracted to: 8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.wav (took 13.47s)\n\nProcessing subject: BST\n  No video files (.mp4, .mkv) found in BST.\n\nProcessing subject: TBT\n  Extracting audio from: 6a_2020-09-22 09-49-32_TBTCon.mkv\n  Successfully extracted to: 6a_2020-09-22 09-49-32_TBTCon.wav (took 6.38s)\n  Extracting audio from: 6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.mkv\n  Successfully extracted to: 6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.wav (took 7.89s)\n\nProcessing subject: Multimodal\n  Extracting audio from: 19853_shylaja.sharath_31_20250401112700078_Video_ENC.mp4\n  Successfully extracted to: 19853_shylaja.sharath_31_20250401112700078_Video_ENC.wav (took 2.36s)\n\nProcessing subject: binary_tree_traversal\n  Extracting audio from: 5a_2020-09-15 09-04-51_BinTraversal.mkv\n  Successfully extracted to: 5a_2020-09-15 09-04-51_BinTraversal.wav (took 8.71s)\n\nProcessing subject: Tree_traversal\n  Extracting audio from: 9a_2020-09-16 09-46-13_TreeTravCon.mkv\n  Successfully extracted to: 9a_2020-09-16 09-46-13_TreeTravCon.wav (took 5.31s)\n\n--- Audio Extraction Summary ---\nTotal videos found: 15\nSuccessfully extracted audio: 15\nFailed extractions: 0\n\nExample output audio files:\n- agentic/audio/19853_shylaja.sharath_31_20250401121200417_Video_ENC.wav\n- TBT/audio/6a_2020-09-22 09-49-32_TBTCon.wav\n- TBT/audio/6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.wav\n- stable_diffusion/audio/19853_shylaja.sharath_31_20250327084200249_Video_ENC.wav\n- stable_diffusion/audio/19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).wav\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Step 3 & 4: Setting Up the Transcription Model (Whisper via Hugging Face)","metadata":{}},{"cell_type":"code","source":"# Step 3 & 4 (Modified): Install Libraries and Configure MEDIUM Whisper Pipeline\n\nimport os\nimport subprocess\nimport torch\nfrom pathlib import Path\n\n# 1. Install Hugging Face libraries (if kernel restart cleared them)\n# It's usually safe to run this again.\nprint(\"Ensuring Hugging Face libraries are installed: transformers, datasets, accelerate, soundfile...\")\ninstall_libs = subprocess.run([\n    'pip', 'install', '-q',\n    'transformers>=4.30.0',\n    'datasets>=2.14.0',\n    'accelerate>=0.21.0',\n    'soundfile',\n    'torch',\n    'sentencepiece',\n    'protobuf'\n], capture_output=True, text=True)\n\nif install_libs.returncode == 0:\n    print(\"Libraries installed/verified successfully.\")\nelse:\n    print(\"Error installing libraries:\")\n    print(install_libs.stderr)\n    raise RuntimeError(\"Failed to install required Hugging Face libraries.\")\n\n# Import necessary components AFTER installation\nfrom transformers import pipeline\nimport soundfile as sf\nimport gc\n\n# 2. Verify GPU again and define device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"\\nGPU confirmed: {gpu_name}. Using device: {device}\")\n    torch_dtype = torch.float16\n    print(f\"Using torch dtype: {torch_dtype}\")\nelse:\n    device = torch.device(\"cpu\")\n    torch_dtype = torch.float32\n    print(\"Warning: GPU not available. Using CPU. Transcription will be VERY slow.\")\n    print(f\"Using torch dtype: {torch_dtype}\")\n\n# 3. Configure and load the ASR pipeline (MEDIUM MODEL)\n# ************************************************\n# Changed model_id to whisper-medium\nmodel_id = \"openai/whisper-medium\"\n# ************************************************\nprint(f\"\\nLoading ASR pipeline for model: {model_id}\")\nprint(\"This may take a few minutes to download the model...\")\n\n# Clear any previously loaded model from memory explicitly\npipe = None\ngc.collect()\ntorch.cuda.empty_cache()\n\ntry:\n    pipe = pipeline(\n        \"automatic-speech-recognition\",\n        model=model_id,\n        torch_dtype=torch_dtype,\n        device_map=\"auto\", # Still use accelerate for optimization\n    )\n    print(\"\\nASR Pipeline loaded successfully.\")\n\n    # Define generation arguments (same as before)\n    generate_kwargs = {\n        \"language\": \"english\",\n        \"task\": \"transcribe\",\n        \"return_timestamps\": True, # Get segment-level timestamps\n    }\n    print(f\"Transcription settings (generate_kwargs): {generate_kwargs}\")\n\nexcept Exception as e:\n    print(f\"\\nError loading pipeline for model {model_id}: {e}\")\n    print(\"Possible issues:\")\n    print(\"- Insufficient GPU memory (try 'whisper-small').\")\n    print(\"- Network issues downloading the model.\")\n    print(\"- Compatibility issues between libraries.\")\n    pipe = None # Ensure pipe is None if loading fails\n\n# --- Placeholder for actual transcription loop (Next Step) ---\nif pipe:\n    print(\"\\nSetup complete. Ready to attempt transcription with the MEDIUM model.\")\nelse:\n    print(\"\\nSetup failed. Cannot proceed with transcription.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:43:25.903958Z","iopub.execute_input":"2025-04-22T05:43:25.904615Z","iopub.status.idle":"2025-04-22T05:43:55.917108Z","shell.execute_reply.started":"2025-04-22T05:43:25.904586Z","shell.execute_reply":"2025-04-22T05:43:55.916402Z"}},"outputs":[{"name":"stdout","text":"Ensuring Hugging Face libraries are installed: transformers, datasets, accelerate, soundfile...\nLibraries installed/verified successfully.\n","output_type":"stream"},{"name":"stderr","text":"2025-04-22 05:43:32.902615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745300612.924942     926 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745300612.931859     926 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\nGPU confirmed: Tesla T4. Using device: cuda\nUsing torch dtype: torch.float16\n\nLoading ASR pipeline for model: openai/whisper-medium\nThis may take a few minutes to download the model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.99k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aaa1710e495421e9342b1cba66757aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27d90e69b78445df958484339d1bbbd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41055ebc67e14fddb585f39ff6e11080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"721149793af744fb9b72a716440ffc43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b2a2a160d714874b7c0d6b26cf46fa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a7c6fb654854129b263c47232054641"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54a8f4fa09ad4225832f68550d8bcaec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3633115498e40438843297cda3f770a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69a9e31a4d954d9f8aec479ff3e55b3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d6a9e6fa6fd4b7098bfefac25e9ba89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192d16f61083448aa081b68204cbe0ce"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\nASR Pipeline loaded successfully.\nTranscription settings (generate_kwargs): {'language': 'english', 'task': 'transcribe', 'return_timestamps': True}\n\nSetup complete. Ready to attempt transcription with the MEDIUM model.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Step 5: Video Transcription Component - Performing Transcription","metadata":{}},{"cell_type":"code","source":"# Step 5: Transcription Loop\n\nimport time\nimport soundfile as sf\nimport numpy as np\nimport gc\nimport torch\nfrom pathlib import Path\n# Assuming 'pipe', 'output_base_path', and 'generate_kwargs' are defined\n# from the previous cell (Step 3 & 4)\n\n# Define where the audio files are and where transcripts should go\naudio_input_base = output_base_path # Contains subject folders -> audio subfolders\ntranscript_output_base = output_base_path # We'll create transcripts subfolders here\n\n# --- Transcription Function ---\ndef format_timestamp(seconds):\n    \"\"\"Converts seconds to HH:MM:SS.ms format\"\"\"\n    if seconds is None:\n        return \"N/A\"\n    millisec = int((seconds - int(seconds)) * 1000)\n    seconds = int(seconds)\n    hours = seconds // 3600\n    minutes = (seconds % 3600) // 60\n    seconds = seconds % 60\n    return f\"{hours:02}:{minutes:02}:{seconds:02}.{millisec:03}\"\n\ndef transcribe_audio_file(audio_path, transcript_path, pipeline, gen_kwargs):\n    \"\"\"Transcribes a single audio file and saves the formatted result.\"\"\"\n    global pipe # Ensure we are using the globally loaded pipeline\n    if not pipe:\n      print(f\"  Skipping {audio_path.name}, pipeline not available.\")\n      return False, 0.0 # Indicate failure and duration 0\n\n    print(f\"  Starting transcription for: {audio_path.name}\")\n    start_time = time.time()\n    try:\n        # Check if audio file is valid and get info (optional but good practice)\n        try:\n            audio_info = sf.info(str(audio_path))\n            if audio_info.samplerate != 16000:\n                 print(f\"    Warning: Sample rate is {audio_info.samplerate}Hz, expected 16000Hz. Model might perform suboptimally.\")\n            if audio_info.channels != 1:\n                 print(f\"    Warning: Audio has {audio_info.channels} channels, expected 1 (mono). Model might perform suboptimally.\")\n        except Exception as e:\n            print(f\"    Warning: Could not read audio file info for {audio_path.name}: {e}\")\n            # Decide whether to continue or skip\n            # return False, 0.0 # Option: Skip if info can't be read\n\n        # Perform transcription - Pipeline can often take the path directly\n        # The pipeline handles loading, chunking, and inference\n        outputs = pipeline(str(audio_path), chunk_length_s=30, stride_length_s=5, generate_kwargs=gen_kwargs, return_timestamps=True) # Ensure timestamps are requested\n\n        full_text = outputs[\"text\"].strip()\n        chunks = outputs.get(\"chunks\", []) # Chunks contain timestamps\n\n        # Format the output\n        output_content = f\"--- Full Transcript ---\\n{full_text}\\n\\n--- Timestamps (Segment/Chunk Level) ---\\n\"\n        if chunks:\n            for i, chunk in enumerate(chunks):\n                start, end = chunk['timestamp']\n                # Sometimes start/end can be None, handle this gracefully\n                start_str = format_timestamp(start) if start is not None else \"???\"\n                end_str = format_timestamp(end) if end is not None else \"???\"\n                chunk_text = chunk['text'].strip()\n                output_content += f\"[{start_str} -> {end_str}] {chunk_text}\\n\"\n        else:\n            output_content += \"No timestamp information available.\\n\"\n\n        # Save the transcript\n        transcript_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(transcript_path, 'w', encoding='utf-8') as f:\n            f.write(output_content)\n\n        end_time = time.time()\n        duration = end_time - start_time\n        print(f\"  Finished transcription for: {audio_path.name} (took {duration:.2f}s)\")\n        print(f\"  Saved transcript to: {transcript_path.relative_to(output_base_path)}\")\n\n        # Clean up GPU memory after processing each file\n        torch.cuda.empty_cache()\n        gc.collect()\n\n        return True, duration\n\n    except FileNotFoundError:\n        print(f\"  Error: Audio file not found at {audio_path}\")\n        return False, 0.0\n    except sf.SoundFileError as e:\n        print(f\"  Error reading audio file {audio_path.name}: {e}\")\n        return False, 0.0\n    except RuntimeError as e:\n        if \"out of memory\" in str(e).lower():\n            print(f\"  Error: CUDA out of memory during transcription for {audio_path.name}.\")\n            print(\"    Try reducing batch size if applicable, using a smaller model, or ensuring GPU has enough VRAM.\")\n            # You might want to release memory explicitly here if possible\n            # del outputs # Try deleting intermediate variables\n            torch.cuda.empty_cache()\n            gc.collect()\n        else:\n            print(f\"  Runtime error during transcription for {audio_path.name}: {e}\")\n        return False, 0.0\n    except Exception as e:\n        print(f\"  An unexpected error occurred during transcription for {audio_path.name}: {e}\")\n        # Print traceback for detailed debugging if needed\n        # import traceback\n        # traceback.print_exc()\n        return False, 0.0\n\n\n# --- Main Transcription Loop ---\ntotal_audio_files = 0\nsuccessful_transcriptions = 0\nfailed_transcriptions = 0\ntotal_transcription_time = 0.0\n\nprint(\"\\n--- Starting Transcription Process ---\")\n\n# Iterate through subject folders in the output directory (where audio was saved)\nfor subject_dir in audio_input_base.iterdir():\n    if subject_dir.is_dir():\n        audio_dir = subject_dir / \"audio\"\n        transcript_dir = transcript_output_base / subject_dir.name / \"transcripts\"\n\n        if audio_dir.exists() and audio_dir.is_dir():\n            print(f\"\\nProcessing audio in: {audio_dir.relative_to(output_base_path)}\")\n            # Find audio files in the current subject's audio directory\n            audio_files = list(audio_dir.glob('*.wav')) # Look specifically for .wav files\n\n            if not audio_files:\n                print(f\"  No .wav files found in {audio_dir}.\")\n                continue\n\n            for audio_file in audio_files:\n                total_audio_files += 1\n                # Define the output transcript file path\n                output_transcript_file = transcript_dir / f\"{audio_file.stem}.txt\"\n\n                # Transcribe the audio file\n                success, duration = transcribe_audio_file(audio_file, output_transcript_file, pipe, generate_kwargs)\n\n                if success:\n                    successful_transcriptions += 1\n                    total_transcription_time += duration\n                else:\n                    failed_transcriptions += 1\n        else:\n             # Check if the subject dir itself contains wav files (if structure differs)\n             audio_files_in_subj = list(subject_dir.glob('*.wav'))\n             if not audio_files_in_subj:\n                print(f\"\\nSkipping subject {subject_dir.name}: No 'audio' subdirectory found and no .wav files in subject root.\")\n\n\nprint(\"\\n--- Transcription Summary ---\")\nprint(f\"Total audio files found: {total_audio_files}\")\nprint(f\"Successfully transcribed: {successful_transcriptions}\")\nprint(f\"Failed transcriptions: {failed_transcriptions}\")\nif successful_transcriptions > 0:\n    avg_time = total_transcription_time / successful_transcriptions\n    print(f\"Total transcription time: {total_transcription_time:.2f}s\")\n    print(f\"Average time per file: {avg_time:.2f}s\")\n\n\n# Optional: Verify by listing some output transcript files\nif successful_transcriptions > 0:\n    print(\"\\nExample output transcript files:\")\n    example_count = 0\n    for subject_dir in transcript_output_base.iterdir():\n        if subject_dir.is_dir() and (subject_dir / \"transcripts\").exists():\n             for transcript_file in (subject_dir / \"transcripts\").iterdir():\n                 if transcript_file.suffix == '.txt' and example_count < 5:\n                     print(f\"- {transcript_file.relative_to(output_base_path)}\")\n                     example_count += 1\n             if example_count >= 5:\n                 break\n    if example_count == 0:\n         print(\"Could not find any example .txt files in the output transcript directories.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:46:40.989285Z","iopub.execute_input":"2025-04-22T05:46:40.990043Z","iopub.status.idle":"2025-04-22T07:21:24.197659Z","shell.execute_reply.started":"2025-04-22T05:46:40.990015Z","shell.execute_reply":"2025-04-22T07:21:24.197056Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Transcription Process ---\n\nProcessing audio in: agentic/audio\n  Starting transcription for: 19853_shylaja.sharath_31_20250401121200417_Video_ENC.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\nYou have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 19853_shylaja.sharath_31_20250401121200417_Video_ENC.wav (took 506.29s)\n  Saved transcript to: agentic/transcripts/19853_shylaja.sharath_31_20250401121200417_Video_ENC.txt\n\nProcessing audio in: TBT/audio\n  Starting transcription for: 6a_2020-09-22 09-49-32_TBTCon.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 6a_2020-09-22 09-49-32_TBTCon.wav (took 283.08s)\n  Saved transcript to: TBT/transcripts/6a_2020-09-22 09-49-32_TBTCon.txt\n  Starting transcription for: 6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\nWhisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.wav (took 333.53s)\n  Saved transcript to: TBT/transcripts/6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.txt\n\nProcessing audio in: stable_diffusion/audio\n  Starting transcription for: 19853_shylaja.sharath_31_20250327084200249_Video_ENC.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 19853_shylaja.sharath_31_20250327084200249_Video_ENC.wav (took 443.33s)\n  Saved transcript to: stable_diffusion/transcripts/19853_shylaja.sharath_31_20250327084200249_Video_ENC.txt\n  Starting transcription for: 19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\nWhisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).wav (took 315.05s)\n  Saved transcript to: stable_diffusion/transcripts/19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).txt\n\nProcessing audio in: BST/audio\n  No .wav files found in /kaggle/working/output/BST/audio.\n\nProcessing audio in: Multimodal/audio\n  Starting transcription for: 19853_shylaja.sharath_31_20250401112700078_Video_ENC.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 19853_shylaja.sharath_31_20250401112700078_Video_ENC.wav (took 510.73s)\n  Saved transcript to: Multimodal/transcripts/19853_shylaja.sharath_31_20250401112700078_Video_ENC.txt\n\nProcessing audio in: binary_tree_traversal/audio\n  Starting transcription for: 5a_2020-09-15 09-04-51_BinTraversal.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 5a_2020-09-15 09-04-51_BinTraversal.wav (took 385.36s)\n  Saved transcript to: binary_tree_traversal/transcripts/5a_2020-09-15 09-04-51_BinTraversal.txt\n\nProcessing audio in: Heap/audio\n  Starting transcription for: 8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_121.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_121.wav (took 424.95s)\n  Saved transcript to: Heap/transcripts/8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_121.txt\n  Starting transcription for: 8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.wav (took 509.65s)\n  Saved transcript to: Heap/transcripts/8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.txt\n\nProcessing audio in: Lora&Qlora/audio\n  Starting transcription for: 19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).wav (took 472.77s)\n  Saved transcript to: Lora&Qlora/transcripts/19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).txt\n","output_type":"stream"},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"  Starting transcription for: 19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).wav (took 485.11s)\n  Saved transcript to: Lora&Qlora/transcripts/19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).txt\n  Starting transcription for: 19853_shylaja.sharath_31_20250318125700082_Video_ENC.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 19853_shylaja.sharath_31_20250318125700082_Video_ENC.wav (took 230.95s)\n  Saved transcript to: Lora&Qlora/transcripts/19853_shylaja.sharath_31_20250318125700082_Video_ENC.txt\n\nProcessing audio in: expn_tree/audio\n  Starting transcription for: 7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.wav (took 204.62s)\n  Saved transcript to: expn_tree/transcripts/7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.txt\n  Starting transcription for: 7a_2020-09-24 09-28-52_ExprTreeCon.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 7a_2020-09-24 09-28-52_ExprTreeCon.wav (took 361.37s)\n  Saved transcript to: expn_tree/transcripts/7a_2020-09-24 09-28-52_ExprTreeCon.txt\n\nProcessing audio in: Tree_traversal/audio\n  Starting transcription for: 9a_2020-09-16 09-46-13_TreeTravCon.wav\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"  Finished transcription for: 9a_2020-09-16 09-46-13_TreeTravCon.wav (took 211.30s)\n  Saved transcript to: Tree_traversal/transcripts/9a_2020-09-16 09-46-13_TreeTravCon.txt\n\n--- Transcription Summary ---\nTotal audio files found: 15\nSuccessfully transcribed: 15\nFailed transcriptions: 0\nTotal transcription time: 5678.11s\nAverage time per file: 378.54s\n\nExample output transcript files:\n- agentic/transcripts/19853_shylaja.sharath_31_20250401121200417_Video_ENC.txt\n- TBT/transcripts/6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198.txt\n- TBT/transcripts/6a_2020-09-22 09-49-32_TBTCon.txt\n- stable_diffusion/transcripts/19853_shylaja.sharath_31_20250327084200249_Video_ENC.txt\n- stable_diffusion/transcripts/19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).txt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Step 6: PowerPoint / PDF Processing Component","metadata":{}},{"cell_type":"code","source":"# Step 6: Presentation Content Extraction (PPTX and PDF)\n\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport gc\n\n# 1. Install necessary libraries\nprint(\"Installing libraries for presentation processing: python-pptx, pdfplumber...\")\n# Pillow is often needed by pdfplumber for image handling, install explicitly\ninstall_pres_libs = subprocess.run([\n    'pip', 'install', '-q',\n    'python-pptx>=0.6.21',\n    'pdfplumber>=0.10.0', # Use a recent version\n    'Pillow'\n], capture_output=True, text=True)\n\nif install_pres_libs.returncode == 0:\n    print(\"Libraries installed successfully.\")\nelse:\n    print(\"Error installing presentation processing libraries:\")\n    print(install_pres_libs.stderr)\n    # Consider raising an error if libraries are essential\n    # raise RuntimeError(\"Failed to install required presentation libraries.\")\n\n# Import AFTER installation\ntry:\n    import pptx\n    import pdfplumber\nexcept ImportError as e:\n    print(f\"Failed to import libraries: {e}. Please ensure installation was successful.\")\n    # Stop execution if imports fail\n    raise\n\n# Define base paths (original data and output)\n# dataset_base_path = Path(\"/kaggle/input/llm-data/LLM_DATASET\") # From Step 1\n# output_base_path = Path(\"/kaggle/working/output\") # From Step 1\n\n# Supported presentation extensions\npresentation_extensions = ['.pptx', '.pdf']\n\n# --- PPTX Processing Function ---\ndef process_pptx(pptx_path):\n    \"\"\"Extracts content (titles, text, notes) from a PPTX file.\"\"\"\n    print(f\"  Processing PPTX: {pptx_path.name}\")\n    presentation_data = {\n        \"source_file\": pptx_path.name,\n        \"file_type\": \"pptx\",\n        \"slides\": []\n    }\n    try:\n        prs = pptx.Presentation(str(pptx_path))\n        for i, slide in enumerate(prs.slides):\n            slide_data = {\n                \"slide_number\": i + 1,\n                \"title\": None,\n                \"content\": [],\n                \"notes\": None\n            }\n\n            # Extract notes if available\n            if slide.has_notes_slide:\n                notes_slide = slide.notes_slide\n                text_frame = notes_slide.notes_text_frame\n                if text_frame:\n                  slide_data[\"notes\"] = text_frame.text.strip() if text_frame.text else None\n\n\n            # Extract shapes' text, trying to identify title and content\n            title_shape = None\n            # Attempt to find title placeholder first\n            if slide.shapes.title:\n                title_shape = slide.shapes.title\n                if title_shape.has_text_frame and title_shape.text.strip():\n                   slide_data[\"title\"] = title_shape.text.strip()\n\n\n            for shape in slide.shapes:\n                # Skip the title shape if already processed\n                if shape == title_shape:\n                    continue\n\n                if shape.has_text_frame:\n                    text = shape.text_frame.text.strip()\n                    if text:\n                       # Simple heuristic: If title not found and shape is high up, maybe it's the title\n                       if slide_data[\"title\"] is None and shape.top < pptx.util.Inches(1.5):\n                           slide_data[\"title\"] = text\n                       else:\n                           # Add paragraphs, preserving some indentation sense for lists\n                           for paragraph in shape.text_frame.paragraphs:\n                               para_text = paragraph.text.strip()\n                               if para_text:\n                                   indent_level = paragraph.level\n                                   prefix = \"  \" * indent_level + \"- \" if indent_level > 0 else \"\"\n                                   slide_data[\"content\"].append(prefix + para_text)\n\n\n            # Fallback if title is still None, use first non-empty content line?\n            if slide_data[\"title\"] is None and slide_data[\"content\"]:\n                # Check if first line looks like a title (e.g., short, no bullet)\n                first_line = slide_data[\"content\"][0]\n                if not first_line.strip().startswith('-'):\n                     slide_data[\"title\"] = first_line\n                     # slide_data[\"content\"].pop(0) # Optional: remove it from content if used as title\n\n            # Clean up empty entries\n            if not slide_data[\"content\"]: slide_data[\"content\"] = None\n            if not slide_data[\"notes\"]: slide_data[\"notes\"] = None\n\n            presentation_data[\"slides\"].append(slide_data)\n\n        return presentation_data\n\n    except Exception as e:\n        print(f\"  Error processing {pptx_path.name}: {e}\")\n        # Optionally return partial data or error indicator\n        # import traceback\n        # traceback.print_exc()\n        return {\"source_file\": pptx_path.name, \"file_type\": \"pptx\", \"error\": str(e)}\n\n\n# --- PDF Processing Function ---\ndef process_pdf(pdf_path):\n    \"\"\"Extracts text content page by page from a PDF file.\"\"\"\n    print(f\"  Processing PDF: {pdf_path.name}\")\n    presentation_data = {\n        \"source_file\": pdf_path.name,\n        \"file_type\": \"pdf\",\n        \"pages\": []\n    }\n    try:\n        with pdfplumber.open(str(pdf_path)) as pdf:\n            for i, page in enumerate(pdf.pages):\n                # Extract text with some tolerance to maintain layout\n                # Adjust tolerance as needed based on PDF structure\n                text = page.extract_text(x_tolerance=2, y_tolerance=3, layout=False, sort_by_position=True)\n                page_data = {\n                    \"page_number\": i + 1,\n                    \"text\": text.strip() if text else None\n                }\n                if page_data[\"text\"]: # Only add pages with actual text content\n                   presentation_data[\"pages\"].append(page_data)\n\n        return presentation_data\n    except Exception as e:\n        print(f\"  Error processing {pdf_path.name}: {e}\")\n        # import traceback\n        # traceback.print_exc()\n        return {\"source_file\": pdf_path.name, \"file_type\": \"pdf\", \"error\": str(e)}\n\n\n# --- Main Processing Loop ---\ntotal_presentations = 0\nprocessed_presentations = 0\nfailed_presentations = 0\n\nprint(\"\\n--- Starting Presentation File Processing ---\")\n\n# Iterate through subject folders in the ORIGINAL dataset directory\nfor subject_dir in dataset_base_path.iterdir():\n    if subject_dir.is_dir():\n        print(f\"\\nProcessing subject: {subject_dir.name}\")\n\n        # Create corresponding output directory structure for presentation data\n        output_subject_pres_dir = output_base_path / subject_dir.name / \"presentation_data\"\n        output_subject_pres_dir.mkdir(parents=True, exist_ok=True)\n\n        # Find presentation files in the current subject directory\n        presentation_files = [f for f in subject_dir.iterdir()\n                              if f.is_file() and f.suffix.lower() in presentation_extensions]\n\n        if not presentation_files:\n            print(f\"  No presentation files ({', '.join(presentation_extensions)}) found in {subject_dir.name}.\")\n            continue\n\n        for pres_file in presentation_files:\n            total_presentations += 1\n            output_json_path = output_subject_pres_dir / f\"{pres_file.stem}_content.json\"\n            extracted_data = None\n\n            if pres_file.suffix.lower() == '.pptx':\n                extracted_data = process_pptx(pres_file)\n            elif pres_file.suffix.lower() == '.pdf':\n                extracted_data = process_pdf(pres_file)\n\n            if extracted_data and \"error\" not in extracted_data:\n                try:\n                    with open(output_json_path, 'w', encoding='utf-8') as f:\n                        json.dump(extracted_data, f, indent=2, ensure_ascii=False)\n                    print(f\"  Saved extracted data to: {output_json_path.relative_to(output_base_path)}\")\n                    processed_presentations += 1\n                except Exception as e:\n                    print(f\"  Error saving JSON for {pres_file.name}: {e}\")\n                    failed_presentations += 1\n            else:\n                print(f\"  Failed to extract data from {pres_file.name}.\")\n                # Check if partial data with error was returned\n                if extracted_data and \"error\" in extracted_data:\n                    print(f\"  Reason: {extracted_data.get('error', 'Unknown error')}\")\n                failed_presentations += 1\n\n            # Clean up memory\n            gc.collect()\n\n\nprint(\"\\n--- Presentation Processing Summary ---\")\nprint(f\"Total presentation files found: {total_presentations}\")\nprint(f\"Successfully processed and saved: {processed_presentations}\")\nprint(f\"Failed processing/saving: {failed_presentations}\")\n\n# Optional: Verify by listing some output JSON files\nif processed_presentations > 0:\n    print(\"\\nExample output JSON files:\")\n    example_count = 0\n    for subject_dir in output_base_path.iterdir():\n        pres_data_dir = subject_dir / \"presentation_data\"\n        if subject_dir.is_dir() and pres_data_dir.exists():\n             for json_file in pres_data_dir.iterdir():\n                 if json_file.suffix == '.json' and example_count < 5:\n                     print(f\"- {json_file.relative_to(output_base_path)}\")\n                     example_count += 1\n             if example_count >= 5:\n                 break\n    if example_count == 0:\n         print(\"Could not find any example .json files in the output presentation_data directories.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T07:40:23.898123Z","iopub.execute_input":"2025-04-22T07:40:23.898670Z","iopub.status.idle":"2025-04-22T07:40:38.228032Z","shell.execute_reply.started":"2025-04-22T07:40:23.898644Z","shell.execute_reply":"2025-04-22T07:40:38.227290Z"}},"outputs":[{"name":"stdout","text":"Installing libraries for presentation processing: python-pptx, pdfplumber...\nLibraries installed successfully.\n\n--- Starting Presentation File Processing ---\n\nProcessing subject: stable_diffusion\n  Processing PDF: Stable Diffusion.pdf\n  Saved extracted data to: stable_diffusion/presentation_data/Stable Diffusion_content.json\n\nProcessing subject: Lora&Qlora\n  Processing PDF: Finetuning.pdf\n  Saved extracted data to: Lora&Qlora/presentation_data/Finetuning_content.json\n\nProcessing subject: expn_tree\n  Processing PPTX: Class7_Unit3_Trees_ExprTree.pptx\n  Saved extracted data to: expn_tree/presentation_data/Class7_Unit3_Trees_ExprTree_content.json\n\nProcessing subject: agentic\n  Processing PDF: AutoGen CrewAI.pdf\n  Saved extracted data to: agentic/presentation_data/AutoGen CrewAI_content.json\n  Processing PDF: Agentic Workflow.pdf\n  Saved extracted data to: agentic/presentation_data/Agentic Workflow_content.json\n\nProcessing subject: Heap\n  Processing PPTX: Class8_Unit3_Trees_Heap.pptx\n  Saved extracted data to: Heap/presentation_data/Class8_Unit3_Trees_Heap_content.json\n\nProcessing subject: BST\n  Processing PPTX: Class2_Unit3_Tree_BST_DynamicInsert.pptx\n  Saved extracted data to: BST/presentation_data/Class2_Unit3_Tree_BST_DynamicInsert_content.json\n  Processing PPTX: Class3_Unit3_Trees_BSTDeletion.pptx\n  Saved extracted data to: BST/presentation_data/Class3_Unit3_Trees_BSTDeletion_content.json\n  Processing PPTX: Class4_Unit3_Trees_BST_ArrayInsert.pptx\n  Saved extracted data to: BST/presentation_data/Class4_Unit3_Trees_BST_ArrayInsert_content.json\n\nProcessing subject: TBT\n  Processing PPTX: Class6_Unit3_Trees_ThreadBST.pptx\n  Saved extracted data to: TBT/presentation_data/Class6_Unit3_Trees_ThreadBST_content.json\n\nProcessing subject: Multimodal\n  Processing PDF: MAMBA.pdf\n  Saved extracted data to: Multimodal/presentation_data/MAMBA_content.json\n  Processing PDF: MultiModal LLMs.pdf\n  Saved extracted data to: Multimodal/presentation_data/MultiModal LLMs_content.json\n\nProcessing subject: binary_tree_traversal\n  Processing PPTX: Class5_Unit3_BST_Traversal.pptx\n  Saved extracted data to: binary_tree_traversal/presentation_data/Class5_Unit3_BST_Traversal_content.json\n\nProcessing subject: Tree_traversal\n  Processing PPTX: class9_Unit3_Trees_naryTraversal.pptx\n  Saved extracted data to: Tree_traversal/presentation_data/class9_Unit3_Trees_naryTraversal_content.json\n\n--- Presentation Processing Summary ---\nTotal presentation files found: 14\nSuccessfully processed and saved: 14\nFailed processing/saving: 0\n\nExample output JSON files:\n- agentic/presentation_data/Agentic Workflow_content.json\n- agentic/presentation_data/AutoGen CrewAI_content.json\n- TBT/presentation_data/Class6_Unit3_Trees_ThreadBST_content.json\n- stable_diffusion/presentation_data/Stable Diffusion_content.json\n- BST/presentation_data/Class2_Unit3_Tree_BST_DynamicInsert_content.json\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Step 7: Content Integration and Note Generation with LLMs","metadata":{}},{"cell_type":"markdown","source":"Step 7.1: Install Groq Library and Initialize Client","metadata":{}},{"cell_type":"code","source":"# Step 7.1 (Modified - Revert to Llama 3 8B): Install Groq Library and Initialize Client\n\nimport os\nimport subprocess\nimport json\nfrom pathlib import Path\nimport time\nimport gc\n\n# 1. Install Groq Python library (run again in case kernel restarted)\nprint(\"Ensuring Groq library is installed...\")\ninstall_groq_lib = subprocess.run(['pip', 'install', '-q', 'groq>=0.4.0'], capture_output=True, text=True)\n\nif install_groq_lib.returncode == 0:\n    print(\"Groq library installed/verified successfully.\")\nelse:\n    print(\"Error installing Groq library:\")\n    print(install_groq_lib.stderr)\n    raise RuntimeError(\"Failed to install Groq library.\")\n\n# Import AFTER installation\ntry:\n    from groq import Groq, RateLimitError, APIError\n    # Using tiktoken to estimate token count for chunking\n    # Install it if not already present\n    try:\n        import tiktoken\n    except ImportError:\n        print(\"Installing tiktoken library for token counting...\")\n        subprocess.run(['pip', 'install', '-q', 'tiktoken'], check=True)\n        import tiktoken\n    print(\"Tiktoken library available.\")\n\n    from kaggle_secrets import UserSecretsClient\nexcept ImportError as e:\n    print(f\"Failed to import libraries: {e}. Please ensure installation was successful.\")\n    raise\n\n# 2. Access API Key from Kaggle Secrets and Initialize Client\ngroq_api_key = None\nclient = None\ntry:\n    user_secrets = UserSecretsClient()\n    groq_api_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n\n    if not groq_api_key:\n        print(\"ERROR: Groq API Key not found in Kaggle Secrets.\")\n        print(\"Please ensure you added a secret with the label 'GROQ_API_KEY'.\")\n    else:\n        print(\"Groq API Key retrieved successfully.\")\n        client = Groq(api_key=groq_api_key)\n        print(\"Groq client initialized.\")\n\nexcept Exception as e:\n    print(f\"Error accessing Kaggle Secrets or initializing Groq client: {e}\")\n    print(\"Ensure Kaggle Secrets are properly configured and the key label is 'GROQ_API_KEY'.\")\n\n# 3. Define LLM Model to use - REVERTING TO LLAMA 3 8B (8k Context)\n# *********************************************************************\n# Reverting to llama3-8b-8192 as large context models failed.\n# We will implement chunking.\nllm_model_name = \"llama3-8b-8192\"\n# *********************************************************************\nprint(f\"Using LLM model: {llm_model_name} (8k context)\")\n\n# --- Placeholder for Note Generation Loop (Next Part) ---\nif client:\n    print(f\"\\nSetup complete. Ready for note generation with {llm_model_name} and chunking.\")\nelse:\n    print(\"\\nSetup failed. Cannot proceed with note generation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:13:48.266820Z","iopub.execute_input":"2025-04-22T08:13:48.267428Z","iopub.status.idle":"2025-04-22T08:13:51.248102Z","shell.execute_reply.started":"2025-04-22T08:13:48.267404Z","shell.execute_reply":"2025-04-22T08:13:51.247340Z"}},"outputs":[{"name":"stdout","text":"Ensuring Groq library is installed...\nGroq library installed/verified successfully.\nTiktoken library available.\nGroq API Key retrieved successfully.\nGroq client initialized.\nUsing LLM model: llama3-8b-8192 (8k context)\n\nSetup complete. Ready for note generation with llama3-8b-8192 and chunking.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"markdown","source":"Step 7.2: Generating Lecture Notes using LLM","metadata":{}},{"cell_type":"code","source":"# Step 7.2 - Cell 1 (Modified for Chunking): Helper Functions, Chunking, Prompt\n\nimport json\nfrom pathlib import Path\nimport time\nimport gc\nimport tiktoken # For token counting / chunking estimate\n# Requires Groq, RateLimitError, APIError from groq (imported in 7.1)\n# Assumes 'client', 'llm_model_name', 'output_base_path' are defined from 7.1\n\n# --- Tokenizer for estimating length ---\n# Use a tokenizer appropriate for Llama 3 models (cl100k_base is common)\ntry:\n    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\nexcept Exception as e:\n     print(f\"Warning: Could not get tiktoken encoder 'cl100k_base'. Using basic split for chunking. Error: {e}\")\n     tokenizer = None\n\n# --- Helper Function for Text Chunking ---\ndef chunk_text(text, max_tokens_per_chunk=2000, overlap_tokens=200):\n    \"\"\"Splits text into chunks based on estimated token count with overlap.\"\"\"\n    if not tokenizer:\n        # Fallback to character-based chunking if tokenizer failed\n        print(\"Warning: Using character-based chunking due to tokenizer issue.\")\n        max_len = max_tokens_per_chunk * 4 # Rough estimate: 4 chars per token\n        overlap_len = overlap_tokens * 4\n        chunks = []\n        start = 0\n        while start < len(text):\n            end = min(start + max_len, len(text))\n            chunks.append(text[start:end])\n            start += max_len - overlap_len\n            if start >= len(text): break # Avoid infinite loop on short overlap\n        return chunks\n\n    # Token-based chunking\n    tokens = tokenizer.encode(text)\n    chunks = []\n    start_token = 0\n    while start_token < len(tokens):\n        end_token = min(start_token + max_tokens_per_chunk, len(tokens))\n        # Decode the chunk tokens back to text\n        chunk_text = tokenizer.decode(tokens[start_token:end_token])\n        chunks.append(chunk_text)\n\n        next_start_token = start_token + max_tokens_per_chunk - overlap_tokens\n        # Ensure we don't get stuck if overlap is too large or chunk is small\n        if next_start_token <= start_token:\n             next_start_token = start_token + 1 # Force progression\n\n        start_token = next_start_token\n        if start_token >= len(tokens): break\n\n    return chunks\n\n\n# --- Helper Function to Format Presentation JSON ---\n# (Keep the format_presentation_data function exactly as it was before)\ndef format_presentation_data(data):\n    \"\"\"Converts presentation JSON data into a readable string for the LLM.\"\"\"\n    if not data or (\"error\" in data):\n        return \"No presentation data available or there was an error processing it.\\n\"\n\n    output = f\"--- Presentation Content ({data['file_type']}: {data['source_file']}) ---\\n\\n\"\n    if data['file_type'] == 'pptx':\n        if not data.get('slides'):\n             return output + \"No slides found or extracted.\\n\"\n        for slide in data['slides']:\n            output += f\"## Slide {slide['slide_number']}\\n\"\n            if slide.get('title'):\n                output += f\"### Title: {slide['title']}\\n\"\n            if slide.get('content'):\n                output += \"Content:\\n\"\n                for line in slide['content']:\n                     output += f\"{line}\\n\"\n            if slide.get('notes'):\n                output += f\"Presenter Notes: {slide['notes']}\\n\"\n            output += \"\\n\"\n    elif data['file_type'] == 'pdf':\n        if not data.get('pages'):\n            return output + \"No pages found or extracted.\\n\"\n        for page in data['pages']:\n            output += f\"## Page {page['page_number']}\\n\"\n            if page.get('text'):\n                output += f\"Text:\\n{page['text']}\\n\"\n            output += \"\\n\"\n    else:\n        output += \"Unknown presentation format.\\n\"\n\n    output += \"--- End of Presentation Content ---\\n\"\n    return output\n\n\n# --- LLM Prompt Template (Slightly Modified for Chunks) ---\n# Added mention that it's a segment/chunk\nprompt_template = \"\"\"\nYou are an expert AI assistant tasked with creating comprehensive, structured lecture notes *for a specific segment* of a lecture.\nYou will be given a transcript segment from the spoken lecture and the content of the *entire* accompanying presentation (slides/PDF).\nYour goal is to synthesize information from BOTH sources to generate high-quality notes *focused on the content covered in the provided transcript segment*, using the presentation for context and structure.\n\n**Instructions:**\n\n1.  **Analyze Both Inputs:** Carefully read the provided Lecture Transcript Segment and the full Presentation Content.\n2.  **Focus on the Segment:** Generate notes *primarily* based on the information discussed in the **Lecture Transcript Segment**.\n3.  **Use Presentation for Context:** Refer to the full Presentation Content to understand the structure, headings, and visual information relevant to the current transcript segment. Integrate points from the slides if they are clearly discussed or referenced *within the segment*.\n4.  **Synthesize Information:** Combine relevant information. Don't just copy; integrate the ideas. Enrich presentation points with details, explanations, and examples mentioned *in the transcript segment*.\n5.  **Structure the Notes:** Organize the notes logically using Markdown formatting:\n    * Use headings (`##`, `###`) for main topics and sub-topics *relevant to this segment*.\n    * Use bullet points (`-` or `*`) for key details, definitions, and examples *from this segment*.\n    * Use bold text (`**text**`) to highlight important keywords, concepts, or definitions *mentioned in this segment*.\n    * Ensure clear paragraph breaks for readability.\n6.  **Output Format:** Produce **Markdown (.md)** formatted notes for *this segment only*. Do not include any introductory or concluding remarks like \"Here are the notes:\" or \"I hope this helps.\". Start directly with the first note heading relevant to the segment. If the segment is very short or contains no substantive information (e.g., only greetings or silence), output only `# [Segment Contains No Substantive Content]`.\n\n**Input Data:**\n\n{presentation_content}\n\n**Lecture Transcript Segment:**\n\n```text\n{transcript_chunk}\n\nGenerated Lecture Notes for this Segment (Markdown Format):\n\"\"\"\n\n# --- Function to call LLM with retry for rate limits ---\n# (Ensure tokenizer is available in the global scope if used here)\ndef generate_notes_with_groq(prompt, max_retries=2, initial_delay=5):\n    \"\"\"Calls the Groq API to generate notes, handles rate limits with retry.\"\"\"\n    global client, llm_model_name, tokenizer # Use global client, model name, and tokenizer\n    if not client:\n        print(\"  Error: Groq client not initialized.\")\n        return None\n\n    delay = initial_delay\n    for attempt in range(max_retries + 1):\n        try:\n            # Estimate prompt tokens BEFORE sending (optional but good practice)\n            prompt_tokens = 0\n            if tokenizer:\n                try:\n                    prompt_tokens = len(tokenizer.encode(prompt))\n                    # print(f\"    Estimated prompt tokens: {prompt_tokens}\") # Debug print\n                    # Check against model limit (leave room for completion tokens)\n                    # Llama3 8k limit (8192). Let's set a safer threshold like 7000.\n                    token_threshold = 7000\n                    if prompt_tokens > token_threshold:\n                        print(f\"  Error: Estimated prompt tokens ({prompt_tokens}) exceed safety margin ({token_threshold}) for {llm_model_name}. Skipping API call.\")\n                        return \"## Error: Content Too Long\\n\\nThe text chunk combined with presentation data exceeded the safe input length for this model. Notes could not be generated for this segment.\"\n                except Exception as e:\n                    print(f\"  Warning: Token estimation failed. Proceeding without check. Error: {e}\")\n\n            # Actual API Call\n            chat_completion = client.chat.completions.create(\n                messages=[\n                    {\n                        \"role\": \"user\",\n                        \"content\": prompt,\n                    }\n                ],\n                model=llm_model_name,\n                temperature=0.5,\n            )\n\n            # Process Response\n            if chat_completion.choices and chat_completion.choices[0].message:\n                response_content = chat_completion.choices[0].message.content\n                if response_content and isinstance(response_content, str) and response_content.strip():\n                    # Success case\n                    return response_content\n                else:\n                    # Handle empty response from LLM\n                    print(f\"  Warning: Received empty content from LLM (Attempt {attempt + 1}/{max_retries + 1}).\")\n                    if attempt < max_retries:\n                        print(f\"   Retrying in {delay} seconds...\")\n                        time.sleep(delay)\n                        delay *= 2\n                        continue # Go to next attempt\n                    else:\n                        return None # Failed after retries\n            else:\n                # Handle invalid response structure\n                print(f\"  Warning: Received invalid response structure from LLM (Attempt {attempt + 1}/{max_retries + 1}).\")\n                if attempt < max_retries:\n                    print(f\"   Retrying in {delay} seconds...\")\n                    time.sleep(delay)\n                    delay *= 2\n                    continue # Go to next attempt\n                else:\n                    return None # Failed after retries\n\n        except RateLimitError as e:\n            # Handle rate limit errors\n            if attempt < max_retries:\n                print(f\"  Warning: Rate limit hit (Attempt {attempt + 1}/{max_retries + 1}). Retrying in {delay} seconds...\")\n                time.sleep(delay)\n                delay *= 2 # Exponential backoff\n            else:\n                print(f\"  Error: Rate limit exceeded after {max_retries} retries. {e}\")\n                return None # Failed after retries\n        except APIError as e:\n            # Handle API errors (including context length)\n            error_message = str(e).lower()\n            if \"request too large\" in error_message or \\\n               \"context_length_exceeded\" in error_message or \\\n               \"maximum context length\" in error_message or \\\n               \"prompt is too long\" in error_message:\n                print(f\"  Error: Input context length exceeded for model {llm_model_name}. Prompt is too long.\")\n                # Check if the error occurred despite passing the initial token check\n                if prompt_tokens <= token_threshold:\n                     print(f\"    Note: Context error occurred even though estimated tokens ({prompt_tokens}) were below threshold ({token_threshold}). Tokenizer mismatch or API limit issue?\")\n                return \"## Error: Content Too Long\\n\\nThe combined transcript and presentation content exceeded the maximum length for this model. Notes could not be generated for this segment.\"\n            else:\n                # Handle other API errors\n                print(f\"  Error: Groq API error (Attempt {attempt + 1}/{max_retries + 1}): {e}\")\n                # Consider retrying certain API errors? For now, fail immediately.\n                return None\n        except Exception as e:\n            # Handle unexpected errors during the call\n            print(f\"  Error: An unexpected error occurred during LLM call (Attempt {attempt + 1}/{max_retries + 1}): {e}\")\n            # import traceback # Uncomment for detailed debugging\n            # traceback.print_exc()\n            if attempt < max_retries:\n                 print(f\"   Retrying in {delay} seconds...\")\n                 time.sleep(delay)\n                 delay *= 2\n                 continue # Go to next attempt\n            else:\n                 return None # Failed after retries\n\n    # Fallback if all retries fail\n    return None\n\n# This print statement should be AFTER the function definition in your cell\nprint(\"Helper functions (incl. chunking), updated prompt template, and LLM caller defined.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:15:51.602336Z","iopub.execute_input":"2025-04-22T08:15:51.602632Z","iopub.status.idle":"2025-04-22T08:15:52.126203Z","shell.execute_reply.started":"2025-04-22T08:15:51.602611Z","shell.execute_reply":"2025-04-22T08:15:52.125432Z"}},"outputs":[{"name":"stdout","text":"Helper functions (incl. chunking), updated prompt template, and LLM caller defined.\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# Step 7.2 - Cell 2 (Modified for Chunking): Main Note Generation Loop\n\n# Initialize counters\ntotal_transcripts_processed_for_notes = 0\nsuccessful_notes_files = 0\nfailed_notes_files = 0\n# Note: Counters now track *files* processed, success/failure determined by processing *all* chunks for a file.\n\nprint(\"\\n--- Starting Lecture Note Generation (with Chunking) ---\")\n\n# Iterate through subject folders in the output directory\nfor subject_dir in output_base_path.iterdir():\n    if not subject_dir.is_dir():\n        continue\n\n    print(f\"\\nProcessing subject: {subject_dir.name}\")\n\n    # --- Load Presentation Data ---\n    presentation_data_dir = subject_dir / \"presentation_data\"\n    presentation_json_files = list(presentation_data_dir.glob('*.json'))\n    if not presentation_json_files:\n        print(f\"  Skipping subject {subject_dir.name}: No presentation JSON file found.\")\n        continue\n    presentation_json_path = presentation_json_files[0]\n    print(f\"  Using presentation data: {presentation_json_path.name}\")\n    try:\n        with open(presentation_json_path, 'r', encoding='utf-8') as f:\n            presentation_data = json.load(f)\n        # Use helper function from Cell 1\n        formatted_presentation = format_presentation_data(presentation_data)\n        if \"error processing it\" in formatted_presentation:\n             print(f\"  Warning: Issue noted during formatting presentation data for {subject_dir.name}.\")\n    except Exception as e:\n        print(f\"  Error loading or formatting presentation JSON {presentation_json_path.name}: {e}\")\n        continue # Skip subject if presentation can't be loaded/formatted\n\n    # --- Find Transcript Files ---\n    transcript_dir = subject_dir / \"transcripts\"\n    transcript_files = list(transcript_dir.glob('*.txt'))\n    if not transcript_files:\n        print(f\"  No transcript files found in {transcript_dir}.\")\n        continue # Move to next subject\n\n    # --- Create Output Directory ---\n    output_notes_dir = subject_dir / \"final_notes\"\n    output_notes_dir.mkdir(parents=True, exist_ok=True)\n\n    # --- Process Each Transcript File (with Chunking) ---\n    for transcript_path in transcript_files:\n        total_transcripts_processed_for_notes += 1\n        print(f\"\\n  Processing transcript file: {transcript_path.name}\")\n        output_note_path = output_notes_dir / f\"{transcript_path.stem}_notes.md\"\n        all_chunk_notes = [] # Store notes generated for each chunk\n        file_had_errors = False # Flag to track if any chunk failed for this file\n\n        # --- Load Transcript Text ---\n        try:\n            with open(transcript_path, 'r', encoding='utf-8') as f:\n                full_content = f.read()\n                transcript_header = \"--- Full Transcript ---\"\n                header_index = full_content.find(transcript_header)\n                if header_index != -1:\n                    text_start_index = full_content.find('\\n', header_index) + 1\n                    timestamp_marker = \"\\n\\n--- Timestamps\"\n                    timestamp_index = full_content.find(timestamp_marker, text_start_index)\n                    transcript_text = full_content[text_start_index:timestamp_index].strip() if timestamp_index != -1 else full_content[text_start_index:].strip()\n                else:\n                    transcript_text = full_content.strip()\n\n            if not transcript_text:\n                 print(\"    Warning: Transcript file is empty. Skipping.\")\n                 failed_notes_files += 1 # Count as failed file\n                 continue # Move to next transcript file\n\n        except Exception as e:\n            print(f\"    Error loading transcript {transcript_path.name}: {e}\")\n            failed_notes_files += 1 # Count as failed file\n            continue # Move to next transcript file\n\n        # --- Chunk the Transcript ---\n        # Use chunk_text function defined in Cell 1\n        # Adjust chunk/overlap tokens as needed. Smaller chunks = more API calls but less likely to hit context limit.\n        # Let's aim for ~3000 tokens combined (prompt+chunk+presentation), so chunk ~1500-2000 tokens?\n        transcript_chunks = chunk_text(transcript_text, max_tokens_per_chunk=1800, overlap_tokens=150)\n        print(f\"    Transcript split into {len(transcript_chunks)} chunks.\")\n\n        # --- Process Each Chunk ---\n        for i, chunk in enumerate(transcript_chunks):\n            print(f\"      Processing chunk {i + 1}/{len(transcript_chunks)}...\")\n            if not chunk.strip():\n                 print(\"      Skipping empty chunk.\")\n                 continue\n\n            # Construct the final prompt for this chunk\n            final_prompt = prompt_template.format(\n                presentation_content=formatted_presentation,\n                transcript_chunk=chunk # Use the current chunk\n            )\n\n            # Call LLM for this chunk\n            # Use generate_notes_with_groq function from Cell 1\n            start_time = time.time()\n            generated_notes_for_chunk = generate_notes_with_groq(final_prompt)\n            end_time = time.time()\n\n            if generated_notes_for_chunk:\n                print(f\"        LLM call for chunk {i + 1} successful (took {end_time - start_time:.2f}s).\")\n                all_chunk_notes.append(generated_notes_for_chunk)\n                # Check if the content indicates an error message was returned\n                if generated_notes_for_chunk.strip().startswith(\"## Error:\"):\n                    print(f\"        NOTE: LLM returned an error message for chunk {i+1}: {generated_notes_for_chunk.splitlines()[0]}\")\n                    file_had_errors = True # Mark file as having errors even if API call itself succeeded\n            else:\n                print(f\"        LLM call failed or returned empty response for chunk {i + 1}.\")\n                # Append an error marker to the notes for this chunk\n                all_chunk_notes.append(f\"\\n\\n## Error: Failed to generate notes for this segment (Chunk {i+1}).\\n\\n\")\n                file_had_errors = True # Mark file as having errors\n\n            # Clean up memory (might be excessive here, but safe)\n            gc.collect()\n            # Optional delay between chunks if needed for rate limits\n            # time.sleep(1)\n\n        # --- Combine Chunk Notes and Save File ---\n        print(f\"    Finished processing all chunks for {transcript_path.name}.\")\n        if all_chunk_notes:\n            # Combine notes from all chunks with separators\n            final_notes_content = \"\\n\\n---\\n\\n\".join(all_chunk_notes) # Add separator between chunks\n\n            # Add a header indicating the source file\n            final_output = f\"# Lecture Notes: {transcript_path.stem}\\n\\n\"\n            if file_had_errors:\n                final_output += \"**Note:** Errors were encountered during the generation of some segments below.\\n\\n---\\n\\n\"\n            final_output += final_notes_content\n\n            try:\n                with open(output_note_path, 'w', encoding='utf-8') as f:\n                    f.write(final_output)\n                print(f\"    Saved combined notes to: {output_note_path.relative_to(output_base_path)}\")\n                if not file_had_errors:\n                    successful_notes_files += 1\n                else:\n                    failed_notes_files += 1 # Count as failed if any chunk had errors\n            except Exception as e:\n                print(f\"    Error saving combined notes to {output_note_path.name}: {e}\")\n                failed_notes_files += 1 # Count as failed if saving fails\n        else:\n            print(f\"    No notes were generated for any chunk of {transcript_path.name}.\")\n            failed_notes_files += 1 # Count as failed if no chunks produced output\n\n\nprint(\"\\n--- Note Generation Loop (with Chunking) Finished ---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:17:24.953406Z","iopub.execute_input":"2025-04-22T08:17:24.954141Z","iopub.status.idle":"2025-04-22T08:18:43.204925Z","shell.execute_reply.started":"2025-04-22T08:17:24.954108Z","shell.execute_reply":"2025-04-22T08:18:43.203930Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting Lecture Note Generation (with Chunking) ---\n\nProcessing subject: agentic\n  Using presentation data: Agentic Workflow_content.json\n\n  Processing transcript file: 19853_shylaja.sharath_31_20250401121200417_Video_ENC.txt\n    Transcript split into 6 chunks.\n      Processing chunk 1/6...\n        LLM call for chunk 1 successful (took 1.40s).\n      Processing chunk 2/6...\n        LLM call for chunk 2 successful (took 49.33s).\n      Processing chunk 3/6...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    998\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_926/41511644.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Use generate_notes_with_groq function from Cell 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mgenerated_notes_for_chunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_notes_with_groq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_926/3687614499.py\u001b[0m in \u001b[0;36mgenerate_notes_with_groq\u001b[0;34m(prompt, max_retries, initial_delay)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;31m# Actual API Call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             chat_completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m    155\u001b[0m                 messages=[\n\u001b[1;32m    156\u001b[0m                     {\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, seed, service_tier, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    320\u001b[0m           \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOverride\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \"\"\"\n\u001b[0;32m--> 322\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    323\u001b[0m             \u001b[0;34m\"/openai/v1/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         )\n\u001b[0;32m-> 1225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    915\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    918\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1006\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/groq/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         return self._request(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":29},{"cell_type":"code","source":"# Step 7.2 - Cell 3: Summary and Verification\n\nprint(\"\\n--- Note Generation Summary ---\")\n# These counters were updated in Cell 2\nprint(f\"Total transcripts found for processing: {total_transcripts_processed}\")\nprint(f\"Successfully generated notes: {successful_notes}\")\nprint(f\"Failed note generations (LLM errors, load/save errors, content too long): {failed_notes}\")\n# Refine skipped count logic if necessary - current counter is basic\n# print(f\"Subjects/Transcripts skipped due to missing inputs: {skipped_due_to_missing_data}\")\n\n# Calculate transcripts skipped (Total - Success - Failed) if needed, assumes total_transcripts_processed is accurate\nprocessed_or_failed = successful_notes + failed_notes\nskipped_transcripts = total_transcripts_processed - processed_or_failed\nif skipped_transcripts < 0 : skipped_transcripts = 0 # Sanity check\nprint(f\"Transcripts skipped or not processed (e.g., empty, load errors before LLM call): {skipped_transcripts}\")\n\n\n# Optional: Verify by listing some output note files\nif successful_notes > 0:\n    print(\"\\nExample output note files (.md):\")\n    example_count = 0\n    for subject_dir in output_base_path.iterdir():\n        notes_data_dir = subject_dir / \"final_notes\"\n        if subject_dir.is_dir() and notes_data_dir.exists():\n             for note_file in notes_data_dir.iterdir():\n                 if note_file.suffix == '.md' and example_count < 5:\n                     print(f\"- {note_file.relative_to(output_base_path)}\")\n                     example_count += 1\n             if example_count >= 5:\n                 break\n    if example_count == 0:\n         print(\"Could not find any example .md files in the output final_notes directories.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T08:11:08.290757Z","iopub.execute_input":"2025-04-22T08:11:08.291230Z","iopub.status.idle":"2025-04-22T08:11:08.298399Z","shell.execute_reply.started":"2025-04-22T08:11:08.291206Z","shell.execute_reply":"2025-04-22T08:11:08.297721Z"}},"outputs":[{"name":"stdout","text":"\n--- Note Generation Summary ---\nTotal transcripts found for processing: 15\nSuccessfully generated notes: 1\nFailed note generations (LLM errors, load/save errors, content too long): 14\nTranscripts skipped or not processed (e.g., empty, load errors before LLM call): 0\n\nExample output note files (.md):\n- agentic/final_notes/19853_shylaja.sharath_31_20250401121200417_Video_ENC_notes.md\n- TBT/final_notes/6a_2020-09-22 09-49-32_TBTCon_notes.md\n- TBT/final_notes/6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_198_notes.md\n- stable_diffusion/final_notes/19853_shylaja.sharath_31_20250327092700214_Video_ENC (1)_notes.md\n- stable_diffusion/final_notes/19853_shylaja.sharath_31_20250327084200249_Video_ENC_notes.md\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}